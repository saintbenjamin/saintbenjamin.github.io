<!-- First -->

<p class="outlined-text-semibig fade-in">
    <em>A Regression-Based Machine Learning Approach to Traces of Inverse Dirac
    Operator</em>
</p>

<p class="outlined-text fade-in">
    The project I have led since my appointment at the Center for Computational
    Sciences, University of Tsukuba, is a research effort to reduce the
    computational cost of measuring physical observables &mdash; traditionally
    calculated using supercomputers &mdash; by replacing them with machine
    learning techniques.
</p>

<p class="outlined-text fade-in">
    Specifically, cumulants (susceptibility, skewness, kurtosis) of the chiral
    condensate, which are computed by leading international research groups such
    as the HotQCD collaboration, are based on traces of the inverse Dirac
    operator: $\mathrm{Tr}\,M^{-1}$, $\mathrm{Tr}\,M^{-2}$,
    $\mathrm{Tr}\,M^{-3}$, and $\mathrm{Tr}\,M^{-4}$. 

    These values are computed stochastically by preparing multiple random sources,
    solving the Dirac equation for each source, and averaging the results. 

    This process requires extensive use of the Conjugate Gradient (CG) algorithm,
    leading to significant computational cost.
</p>

<p class="outlined-text fade-in">
    Accordingly, I aimed to reduce this cost by replacing or supplementing the
    traditional measurement process with machine learning. 

    Specifically, I constructed a supervised $X \to Y$ regression model, where $X$
    represents a low-cost observable that is relatively easy to compute, and $Y$
    represents a high-cost target observable. 

    The entire dataset is divided into a labeled set and an unlabeled set; a portion
    of the labeled set is used as a training set to train the model, and $Y$ is then
    predicted based on the $X$ values in the unlabeled set.
</p>

<p class="outlined-text fade-in">
    An important point is that not all of the labeled set is used for training;
    rather, it is further divided into a training set and a bias correction set, and
    bias correction is performed based on the latter.

    This methodology is based on the bias correction theory presented in
    <a href="https://arxiv.org/abs/1807.05971" target="_blank">Yoon <i>et al.</i></a>,
    which is built upon <a href="https://arxiv.org/abs/1208.4349"
    target="_blank">the All Mode Averaging (AMA) method</a>.
</p>

<div class="outlined-text fade-in">
    <p>
        My particular focus was on building a system that evaluates the stability and
        reliability of machine learning results as two ratios are varied:
    </p>
    <ol class="paren-ol">
        <li>the proportion of the labeled set relative to the full dataset,</li>
        <li>the proportion of the training set within the labeled set.</li>
    </ol>
</div>

<p class="outlined-text fade-in">
    Through this so-called label-train scanning system, I clearly identified regions
    where prediction performance deteriorates sharply due to insufficient labeled
    data, as well as regions where performance remains stable given an adequate
    labeled set.
</p>

<p class="outlined-text fade-in">
    I gave a talk based on this initial research at Lattice 2024 (Liverpool),
    just three months after my appointment, and the results were published as
    <a href="https://arxiv.org/abs/2411.18170v2" target="_blank">Choi <i>et
    al.</i></a>.

    Since then, the code has evolved beyond simply estimating
    $\mathrm{Tr}\,M^{-1}$, $\mathrm{Tr}\,M^{-2}$, $\mathrm{Tr}\,M^{-3}$, and
    $\mathrm{Tr}\,M^{-4}$ using machine learning. 

    It now includes functionality for calculating the chiral condensate
    cumulants for a single ensemble based on these predictions. 

    Furthermore, it has developed into a complete workflow that automatically
    performs multi-ensemble reweighting to find the phase transition point
    (critical $\kappa$), and obtains cumulants through interpolation based on
    that point.
</p>

<div class="outlined-text fade-in">
    <p>
        I have structured the entire research into three integrated and
        automated phases, and assigned the following names to each module:
    </p>
    <ul class="bullet-ul">
        <li><strong><code>Deborah.jl</code></strong>: machine-learning-based
        trace estimation (phase&nbsp;1)</li>

        <li><strong><code>Esther.jl</code></strong>: cumulant calculation at
        single ensemble (phase&nbsp;2)</li>

        <li><strong><code>Miriam.jl</code></strong>: multi-ensemble reweighting
        and interpolation (phase&nbsp;3)</li>
    </ul>
</div>

<p class="outlined-text fade-in">
    Among them, <strong><code>Deborah.jl</code></strong> was mainly used for
    trace estimation, but it was designed from the beginning with a generalized
    structure to flexibly support various types of data. 

    To this end, it supports not only linear regression models such as
    <span class="sc">Lasso</span> and <span class="sc">Ridge</span> but also
    gradient boosting decision trees via LightGBM.
</p>

<p class="outlined-text fade-in">
    I organically integrated these three modules to construct a fully automated
    system that begins with raw data and proceeds all the way to the final
    physical quantity computation. 

    Through this system, I confirmed a significant reduction in computational cost. 

    Based on these research results, I am currently co-authoring a paper with
    Professors Hiroshi Ohno and Akio Tomiya, with the goal of publishing it in
    the near future. 

    The related code will also be released on GitHub concurrently with the
    publication.
</p>

<!-- Second -->

<p class="outlined-text-semibig fade-in">
    <em>Data analysis technique: toward the precise determination of
    $\left|V_{cb}\right|$ using the Oktay-Kronfeld action</em>
</p>

<p class="outlined-text fade-in">
    My collaborators in LANL/SWME and I am having been worked for the
    calculation of For the heavy (light) quark, we are using the
    <a href="https://arxiv.org/abs/0803.0523" target="_blank">Oktay-Kronfeld
    action</a> (<a href="https://arxiv.org/abs/hep-lat/0610092"
    target="_blank">HISQ action</a>). For the sea quark, we use the <a
    href="https://arxiv.org/abs/1212.4768" target="_blank">MILC HISQ
    ensemble</a>.
</p>

<p class="outlined-text fade-in">
    During the Ph.D. course days, I have mainly worked on the data analysis
    technique. For the determination of $\left|V_{cb}\right|$ combining with the
    experimental result, we need to calculate semileptonic form factor such as
    $\bar{B}\to D^{\ast}\ell\bar{\nu}$ at the zero recoil ($w=1$). For it, we
    need to perform the 3pt correlation function fit on the four channels: $B\to
    D^{\ast}$, $D^{\ast}\to B$, $B\to B$, $D^{\ast}\to D^{\ast}$. Here, we do
    not use <a href="https://arxiv.org/abs/0808.2519"
    target="_blank">$\bar{R}^{\frac{1}{2}}$ constant fit method</a>. We use
    linear fit of matrix elements for ground state or excited states, using
    fitting parameters of 2pt correlator fit as input parameters.
</p>

<p class="outlined-text fade-in">
    Hence, we need to perform the 2pt correlator fit first. We use sequential
    Bayesian method. For the least $\chi^{2}$ fitting, we use
    Broyden-Fletcher-Goldfarb-Shanno algorithm, which is a quasi-Newton method.
    Any Newton method need its initial guess. If we do not feed good initial
    guess ($\chi^{2}$-IG) to the $\chi^{2}$ minimizer, then the computing time
    increases. But if we feed good initial guess to the $\chi^{2}$ minimizer,
    then we can save computing time dramatically. By this reason, we
    additionally introduced Newton method to our 2pt correlation function fit.
    Note that Newton method needs its own initial guess (N-IG). To put N-IG
    within the radius of convergence, we further introduce scanning method.
</p>

<p class="outlined-text fade-in">
    To solve the Newton method, we need multiple time slices which is equal to
    the number of fitting parameters. Those time slices can be chosen within the
    fit range. By this, we have multiple time slice combinations. Before the
    least $\chi^{2}$ fitting, we try Newton method for all possible such time
    slice combinations. If the Newton method finds roots, we save them. Or we
    abandon that time slice combination. In the coarse lattice, at the final
    step of the sequential Bayesian method, Newton method find
    $O\left(10^{4}\right)$ roots. In order to monitor statistics for the
    $\chi^{2}$ distribution, we randomly select $O\left(10^{3}\right)$ roots
    among them, for example.
</p>

<p class="outlined-text fade-in">
    By this kind of combination of scanning method, Newton method and sequential
    Bayesian method, we can find global minimum and local minima of $\chi^{2}$,
    consuming up all possible time slices. I updated our fitting code. I
    developed and applied multidimensional iterative scanning method to our
    fitting code. And I applied multidimensional Newton method to our fitting
    code. 
</p>

<p class="outlined-text fade-in">
    Through these accomplishments, I am interested in the combinations of two or
    more various algorithms to compensate each other, such as sequential
    Bayesian method and Newton method.
</p>