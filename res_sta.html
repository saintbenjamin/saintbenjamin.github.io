<!-- First -->

<p class="outlined-text-semibig">
    <em>Machine Learning-based Estimation of the Cumulants of Chiral Condensate using Gradient Boosting Decision Tree Regression</em>
</p>

<p class="outlined-text">
    In Quantum Chromodynamics (QCD), the calculation of higher-order cumulants
    of the chiral condensate is necessary to investigate the location of the
    critical endpoint of the finite-temperature phase transition.
</p>

<p class="outlined-text">
    For example, the kurtosis intersection method is often used to determine the critical endpoint of the finite-temperature phase transition. The calculation of these cumulants of the chiral order parameter involves evaluating $\textrm{Tr} \; M^{-n}$ ($n=1,2,3,4$), i.e., the trace of the inverse Dirac operator raised to the power of up to four. This requires solving a large number of linear equations involving the Dirac matrix.
</p>

<p class="outlined-text">
    In lattice Quantum Chromodynamics (Lattice QCD), the Dirac operator is a
    very large and sparse matrix. Consequently, solving these linear equations
    requires the conjugate gradient method, which incurs a very high
    computational cost.
</p>

<p class="outlined-text">
    Therefore, in this study, we proposed a method to reduce this computational cost by utilizing machine learning to estimate $\mathrm{Tr} M^{-n}$ from more easily computable physical quantities.
</p>

<p class="outlined-text">
    As the estimation method, we adopted a technique that combines gradient boosting decision tree regression and the bias correction approximation method <a href="https://arxiv.org/abs/1605.04659" target="_blank">proposed by Yoon et al.</a>.

    Furthermore, as proof-of-concept data, we used lattice QCD simulation results from <a href="https://doi.org/10.22323/1.334.0174" target="_blank">a previous study</a>, which employed the Iwasaki gauge action and an $O(a)$-improved Wilson quark action with four degenerate flavors.
</p>

<p class="outlined-text">
    In this study, supervised learning was performed as the machine learning approach. The original dataset was divided into two main parts: a labeled dataset and an unlabeled dataset. The labeled dataset was further split into a training set and a bias correction set.
</p>

<p class="outlined-text">
    The training set was used exclusively to construct the model for gradient boosting decision tree regression. The trained machine learning model was then used to estimate values for the unlabeled dataset, and bias correction approximation was applied.

    To improve statistical accuracy, we also computed the weighted average of the previously obtained results and the labeled dataset.
</p>

<p class="outlined-text">
    Additionally, we investigated how the estimation accuracy of $\textrm{Tr} \; M^{-n}$ ($n=2,3,4$) changed by varying the proportion of the labeled dataset and the training set within it.

    Here, $\textrm{Tr} \; M^{-1}$ was directly taken from the original dataset as input.
</p>

<p class="outlined-text">
    Subsequently, the susceptibility, skewness, and kurtosis of the chiral condensate were computed and compared with the results obtained using the original dataset.
</p>

<p class="outlined-text">
    The results showed that, for all the validation datasets used, even when the labeled dataset comprised only about $5\%$ of the total data, the susceptibility, skewness, and kurtosis all closely matched the values obtained from the original dataset.
</p>

<p class="outlined-text">
    Therefore, it is possible to compute various cumulants, including kurtosis, with the same accuracy as the original dataset while requiring only about $(100+5+5+5)/400 \approx 28.75 \%$ of the original computational cost.
</p>



<!-- Second -->

<p class="outlined-text-semibig">
    <em>Data analysis technique: toward the precise determination of
    $\left|V_{cb}\right|$ using the Oktay-Kronfeld action</em>
</p>

<p class="outlined-text">
    My collaborators in LANL/SWME and I am having been worked for the
    calculation of For the heavy (light) quark, we are using the
    <a href="https://arxiv.org/abs/0803.0523" target="_blank">Oktay-Kronfeld action</a> (<a href="https://arxiv.org/abs/hep-lat/0610092" target="_blank">HISQ action</a>).
    For the sea quark, we use the <a href="https://arxiv.org/abs/1212.4768" target="_blank">MILC HISQ ensemble</a>.
</p>

<p class="outlined-text">
    During the Ph.D. course days, I have mainly worked on the data
    analysis technique. For the determination of $\left|V_{cb}\right|$
    combining with the experimental result, we need to calculate
    semileptonic form factor such as $\bar{B}\to D^{\ast}\ell\bar{\nu}$
    at the zero recoil ($w=1$). For it, we need to perform the 3pt
    correlation function fit on the four channels: $B\to D^{\ast}$,
    $D^{\ast}\to B$, $B\to B$, $D^{\ast}\to D^{\ast}$. Here, we do not
    use <a href="https://arxiv.org/abs/0808.2519"
    target="_blank">$\bar{R}^{\frac{1}{2}}$ constant fit method</a>. We
    use linear fit of matrix elements for ground state or excited
    states, using fitting parameters of 2pt correlator fit as input
    parameters.
</p>

<p class="outlined-text">
    Hence, we need to perform the 2pt correlator fit first. We use
    sequential Bayesian method. For the least $\chi^{2}$ fitting, we use
    Broyden-Fletcher-Goldfarb-Shanno algorithm, which is a quasi-Newton
    method. Any Newton method need its initial guess. If we do not feed
    good initial guess ($\chi^{2}$-IG) to the $\chi^{2}$ minimizer, then
    the computing time increases. But if we feed good initial guess to
    the $\chi^{2}$ minimizer, then we can save computing time
    dramatically. By this reason, we additionally introduced Newton
    method to our 2pt correlation function fit. Note that Newton method
    needs its own initial guess (N-IG). To put N-IG within the radius of
    convergence, we further introduce scanning method.
</p>

<p class="outlined-text">
    To solve the Newton method, we need multiple time slices which is
    equal to the number of fitting parameters. Those time slices can be
    chosen within the fit range. By this, we have multiple time slice
    combinations. Before the least $\chi^{2}$ fitting, we try Newton
    method for all possible such time slice combinations. If the Newton
    method finds roots, we save them. Or we abandon that time slice
    combination. In the coarse lattice, at the final step of the
    sequential Bayesian method, Newton method find
    $O\left(10^{4}\right)$ roots. In order to monitor statistics for the
    $\chi^{2}$ distribution, we randomly select $O\left(10^{3}\right)$
    roots among them, for example.
</p>

<p class="outlined-text">
    By this kind of combination of scanning method, Newton method and
    sequential Bayesian method, we can find global minimum and local
    minima of $\chi^{2}$, consuming up all possible time slices. I
    updated our fitting code. I developed and applied multidimensional
    iterative scanning method to our fitting code. And I applied
    multidimensional Newton method to our fitting code. 
</p>

<p class="outlined-text">
    Through these accomplishments, I am interested in the combinations
    of two or more various algorithms to compensate each other, such as
    sequential Bayesian method and Newton method.
</p>